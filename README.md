# Creating a Queryable 247 Sports College Football Recruiting Database 
This project is a set of Python and shell scripts to fetch and process publicly available data from 247 for non-commercial, personal data analysis use to be done using AWS Athena. Please note that the code is intended to be relatively transient. If any of the HTML changes on the site, the scraper will need to be modified.

## Stage One: Fetching Recruit Lists by Year
Players are scraped from the recruiting index page in the following format:
```json
{
    "247_id": "46038819",
    "247_url": "https://247sports.com/Player/Bryan-Bresee-46038819",
    "full_name": "Bryan Bresee",
    "year": 2020,
    "position": "DT",
    "high_school": "Damascus",
    "city": "Damascus",
    "state": "MD",
    "score": "0.9995",
    "stars": 5,
    "height_feet": 6.0,
    "height_inches": 5.0,
    "weight": 290.0
}
``` 
There's also some basic exception handling to insert default values for inconsistent or missing data.
![Error handling](screenshots/recruit-list-error-handling.png)

Before running the initial script, be sure to change the year range to fetch for in `scrape_recruit_list.py`:
```python
year_range = range(2010, 2021)
```

All dependencies are located in `requirements.txt`. To run, simple execute the command `python scrape_recruit_list.py`. The script will generate a file for each year (i.e. `recruit-list-2020.json`) in the `/recruit-lists` directory. These directories are treated as output and ignored via the `.gitignore`.

## Stage Two: Obtaining Ranking History and Recruiting Timeline Events
With a set of lists generated by stage one, the `process_recruits.py` script fetches and parses the complete ranking history and events (i.e. official visits, offers, etc.). To run, pass a recruiting list from stage one and the corresponding year to produce the files: `python process_recruits.py 2020 recruit-lists/recruit-list-2020.json`. 247's data isn't perfectly formatted, so stage three fixes issues from stage two and formats for more efficient querying using AWS Athena.
![Error handling](screenshots/process-recruits-error-handling.png)

Recruit ranking histories are stored in the following path: `/recruit-ranking-histories/{year}/{247_id}.json`. For example, Bryan Bresee's path would be `/recruit-ranking-histories/2020/46038819.json` in the following format:
```json
{
    "247_id": "46038819",
    "rating": 0.9995,
    "rank": 1,
    "change_date": "2020-01-31",
    "delta": -0.0002,
    "delta_inception_value": 0.0295
}
```

Recruiting timeline events are stored in the following path: `/recruit-timeline-histories/{year}/{247_id}.json`. For example, Bryan Bresee's path would be `/recruit-timeline-histories/2020/46038819.json` in the following format:
```json
{
    "247_id": "46038819",
    "event_date": "2020-01-08",
    "event_type": "Enrollment",
    "event_description": "Bryan Bresee enrolls at Clemson Tigers"
}
```

Given the large amount of data to process during stage two, this project also includes a bootstrapping script for EC2 instances to install the Python tooling, configure the virtual environment, and pull the data from stage one via S3:
```shell script
#!/bin/bash
sudo yum install git -y
sudo yum install python3 -y
git clone https://github.com/scottenriquez/247-recruiting-ranking-history-scraper.git
cd 247-recruiting-ranking-history-scraper
mkdir recruit-lists
mkdir recruit-ranking-histories
mkdir recruit-timeline-histories
aws s3 cp s3://247-recruit-rankings-2010-2020/recruit-list/ recruit-lists --recursive
python3 -m venv env
source env/bin/activate
sudo pip3 install -r requirements.txt
```

Note that since S3 bucket names are globally unique, this will need to be changed for any other bucket.

## Stage Three: Cleanup and Optimization
After the first two stages, there should be three directories of data:
- `/recruit-lists` contains one file per year containing all recruits from that year
- `/recruit-ranking-histories` contains subdirectories for each year storing an individual JSON file per recruit capturing ranking changes
- `/recruit-timeline-histories` contains subdirectories for each year storing an individual JSON file per recruit capturing events like official visits

Several scrips to format this raw data are maintained in this repository. The first of these is `merge_utility.py` which merges all files in each of the top level directories into unified lists. This can be easier to manage than handling the thousands of files generated by the application depending on use case. Specifically it is more peformant for AWS Athena which prefers larger files as opposed to a higher volume of files. To run, use the command `python merge_utility.py <PATH_TO_DIRECTORY_WITH_FILES_TO_MERGE> <PATH_TO_OUTPUT_FILE>`.

Numerous duplicate recruits exist after producing the recruit lists in stage one, so `duplicate_utility.py` can be run to clean a stage one file in place: `python duplicate_utility.py <PATH_TO_RECRUIT_LIST_FILE>`.

## Configuring AWS Athena
WIP
